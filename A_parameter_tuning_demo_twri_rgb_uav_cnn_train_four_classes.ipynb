{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hniu-tamu/cotton_water_stress_remote_sensing/blob/main/A_parameter_tuning_demo_twri_rgb_uav_cnn_train_four_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uOFEdT4ZHVGf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import sys\n",
        "import numpy\n",
        "import os\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "numpy.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "9UCIuUe23qIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f252ebe-a13b-49f6-a179-749cb4db3bd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQWGpi1JHoOg",
        "outputId": "4487af8d-4733-4292-9148-a634f2e4a29f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9t4YQ194HtRA",
        "outputId": "a68a8965-96d7-424f-e497-ac76f381b148"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq1MsYmQb-sc",
        "outputId": "c32bf319-7ed4-434a-9245-7e8b4d10ba6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"CNN images\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "b-5byivMIbMX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  true_label, img = true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],  # minus 1 because 1,2,3,4\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]), # minus 1 because 1,2,3,4\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  true_label = true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(4))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(4), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9J7hCEYHwBu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras==0.1.8"
      ],
      "metadata": {
        "id": "6KhFuK68hDqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(neurons=64, kernel_size=(3, 3), pool_size=(2, 2), optimizer='adam'):\n",
        "      model = models.Sequential()\n",
        "      model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)))\n",
        "      model.add(layers.MaxPooling2D(pool_size))\n",
        "      model.add(layers.Conv2D(neurons, kernel_size, activation='relu'))\n",
        "      model.add(layers.MaxPooling2D(pool_size))\n",
        "      model.add(layers.Conv2D(neurons, kernel_size, activation='relu'))\n",
        "      model.add(layers.Flatten())\n",
        "      model.add(layers.Dense(neurons, activation='relu'))\n",
        "      model.add(layers.Dense(4))\n",
        "      # model.summary()\n",
        "      # compile the model\n",
        "      model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "      return model"
      ],
      "metadata": {
        "id": "QpdFsevZPP9s"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for date in [\"8_18\"]: # \"8_18\", \"9_02\", \"9_09\", \"9_20\"\n",
        "  for img_size in [64]: # 32, 64, 128\n",
        "    # load the data\n",
        "    data = np.load(f'/content/drive/MyDrive/Colab Notebooks/twri_rgb_6832_cotton_{img_size}x{img_size}_{date}.npy')\n",
        "    labels = pd.read_csv('labels_rgb.csv')\n",
        "    y_label = labels['class'].values\n",
        "\n",
        "    # remove the filler type\n",
        "    new_data_without_filler = np.zeros((5376, img_size, img_size, 3)) # defult was 32,32,3\n",
        "    new_y_label_without_filler = np.zeros(5376, dtype=int)\n",
        "    i = 0\n",
        "    for idx, val in enumerate(y_label):\n",
        "      if val != 0:\n",
        "        new_data_without_filler[i] = data[idx]\n",
        "        new_y_label_without_filler[i] = y_label[idx]\n",
        "        i+=1\n",
        "    data = new_data_without_filler\n",
        "    y_label = new_y_label_without_filler - 1\n",
        "\n",
        "    # split the data and normalization\n",
        "    train_images, test_images, train_labels, test_labels = train_test_split(data, y_label, test_size=0.3, random_state=42)\n",
        "    train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "    class_names = ['rainfed', 'fully irrigated', 'percent deficit', 'time delay']\n",
        "\n",
        "    # Define the hyperparameters to search\n",
        "    param_grid = {\n",
        "      'optimizer': ['adam', 'sgd'],\n",
        "      'neurons': [32, 64, 128],\n",
        "      'kernel_size': [(3, 3), (4, 4)],\n",
        "      'pool_size': [(2, 2), (3, 3)]\n",
        "    }\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_hyperparameters = {}\n",
        "    for optimizer, neurons, kernel_size, pool_size in product(param_grid['optimizer'],\n",
        "                                                         param_grid['neurons'],\n",
        "                                                         param_grid['kernel_size'],\n",
        "                                                         param_grid['pool_size']):\n",
        "      model = create_model(neurons=neurons,\n",
        "                         kernel_size=kernel_size,\n",
        "                         pool_size=pool_size,\n",
        "                         optimizer=optimizer)\n",
        "\n",
        "      history = model.fit(train_images, train_labels, epochs=70,\n",
        "                        validation_data=(test_images, test_labels))\n",
        "\n",
        "      # plot and save the image\n",
        "      # plt.figure(figsize=(8,6))\n",
        "      # plt.plot(history.history['accuracy'], label='accuracy')\n",
        "      # plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "      # plt.xlabel('Epoch')\n",
        "      # plt.ylabel('Accuracy')\n",
        "      # plt.ylim([0.5, 1.2])\n",
        "      # plt.legend(loc='lower right')\n",
        "      #save_fig(f\"Training and validation accuracy for {img_size} x {img_size} at {date} with four classes\")\n",
        "\n",
        "      # Print the current hyperparameters\n",
        "      print(f\"Optimizer: {optimizer}, Neurons: {neurons}, Kernel Size: {kernel_size}, Pool Size: {pool_size}\")\n",
        "\n",
        "      test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "      if test_acc > best_accuracy:\n",
        "        best_accuracy = test_acc\n",
        "        best_hyperparameters = {\n",
        "            'optimizer': optimizer,\n",
        "            'neurons': neurons,\n",
        "            'kernel_size': kernel_size,\n",
        "            'pool_size': pool_size\n",
        "        }\n",
        "\n",
        "      probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "      predictions = probability_model.predict(test_images)\n",
        "      pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "      # # save the confusion matrix image\n",
        "      # tf.math.confusion_matrix(test_labels, pred)\n",
        "      # ConfusionMatrixDisplay.from_predictions(test_labels, pred)\n",
        "      #save_fig(f\"Confusion matrix for {img_size} x {img_size} at {date} with four classes\")\n",
        "\n",
        "      # save the precision and recall information to .txt files\n",
        "\n",
        "      precision_recall_fscore_support(test_labels, pred, labels=[0, 1])\n",
        "      #target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
        "      target_names = ['rainfed', 'fully irrigated', 'percent deficit', 'time delay']\n",
        "      print(classification_report(test_labels, pred, target_names=target_names))\n",
        "      report = classification_report(test_labels, pred, target_names=target_names)\n",
        "      report_path = f\"Precision and Recall for {img_size} x {img_size} at {date} with four classes.txt\"\n",
        "      text_file = open(report_path, \"w\")\n",
        "      n = text_file.write(report)\n",
        "      text_file.close()\n",
        "\n",
        "        # Print the best hyperparameters and test accuracy\n",
        "    print(\"\\nBest Hyperparameters:\")\n",
        "    print(best_hyperparameters)\n",
        "    print(\"Best Test Accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "id": "HPG40cJpHw8G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}